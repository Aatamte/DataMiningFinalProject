wandb: Detected [openai] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
wandb enabled: DataMiningSLMSearch/train_20251203_221426

[1/4] Loading questions...

[2/4] Loading model: Qwen/Qwen2.5-3B-Instruct...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|##########| 2/2 [00:04<00:00,  2.25s/it]
  Device: cuda
  Parameters: 3,085,938,688

[3/4] Setting up judge...
  Judge model: qwen2.5:7b
  Training samples: 10

[4/4] Training for 1 epoch(s)...

--- Epoch 1/1 ---

  Q1: What controversy involved the Fallout 76 Power Armor edition...
  Expected: Canvas bag replaced with nylon...
    Rollout 1: reward=0, answer='Based on the search results, n...'
    Rollout 2: reward=0, answer='Based on the previous searches...'
    Loss: 1.6877

  Q2: What is the name of the demon possessing the death-row inmat...
  Expected: Nefarious...
    Rollout 1: reward=0, answer='It appears there was an error ...'
    Rollout 2: reward=0, answer='convinced when Nefarious revea...'
Traceback (most recent call last):
  File "V:\DataMining2025\scripts\train.py", line 86, in <module>
    main()
    ~~~~^^
  File "V:\DataMining2025\scripts\train.py", line 82, in main
    asyncio.run(main_async())
    ~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Users\aaron\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "C:\Users\aaron\AppData\Local\Programs\Python\Python313\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "C:\Users\aaron\AppData\Local\Programs\Python\Python313\Lib\asyncio\base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "V:\DataMining2025\scripts\train.py", line 77, in main_async
    await trainer.train(live_plot=args.live_plot)
  File "V:\DataMining2025\src\trainer\core.py", line 235, in train
    loss = compute_reinforce_loss(
        self.model, self.tokenizer, episodes, rewards, self.device
    )
  File "V:\DataMining2025\src\trainer\episode.py", line 119, in compute_reinforce_loss
    outputs = model(**inputs, labels=inputs["input_ids"])
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\utils\generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 449, in forward
    outputs: BaseModelOutputWithPast = self.model(
                                       ~~~~~~~~~~^
        input_ids=input_ids,
        ^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\utils\generic.py", line 1072, in wrapper
    outputs = func(self, *args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 384, in forward
    hidden_states = decoder_layer(
        hidden_states,
    ...<6 lines>...
        **kwargs,
    )
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\utils\deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 234, in forward
    hidden_states, _ = self.self_attn(
                       ~~~~~~~~~~~~~~^
        hidden_states=hidden_states,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "V:\DataMining2025\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\utils\deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 169, in forward
    attn_output, attn_weights = attention_interface(
                                ~~~~~~~~~~~~~~~~~~~^
        self,
        ^^^^^
    ...<7 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "V:\DataMining2025\.venv\Lib\site-packages\transformers\integrations\sdpa_attention.py", line 96, in sdpa_attention_forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
        query,
    ...<6 lines>...
        **sdpa_kwargs,
    )
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 53.81 GiB is allocated by PyTorch, and 457.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
