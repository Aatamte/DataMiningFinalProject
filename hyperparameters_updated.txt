The base model is Qwen3-4B-Instruct-2507 (4 billion parameters), fine-tuned using Low-Rank Adaptation (LoRA) with rank 8, alpha 16, and dropout 0.05 on the Q, K, V, and O attention projections, with gradient checkpointing enabled. We train for 10 epochs over 300 questions with 20 rollouts per question, sampling temperature uniformly from 0.7 to 1.15 to induce trajectory diversity. Each episode runs for up to 2 turns with a maximum of 256 new tokens per generation and a 3000-token context window. The optimizer uses learning rate 1e-4, gradient clipping at max norm 1.0, and temporal discount Î³=0.99 for earlier turns. The reward weights correctness at 75% and approach quality at 25%, optimized using GRPO. A Qwen3-8B-AWQ judge model evaluates trajectories via a local API endpoint.
