# Full training run - production settings

model:
  name: "Qwen/Qwen3-4B-Instruct-2507"
  max_context: 3000  # Reduced from 3000 to save GPU memory
  use_tf32: true      # TF32 for Ampere GPUs (~10-15% faster)
  attention: "sdpa"   # "eager", "sdpa" (recommended), or "flash_attention_2"
  use_torch_compile: false  # torch.compile disabled (triton compatibility issues)

training:
  num_samples: 300
  num_epochs: 10
  num_rollouts: 24
  batch_splits: 1  # -1 = sequential, N = split rollouts into N batches for turn 1
  gradient_micro_batches: 4  # Split rollouts into N micro-batches for backward() - saves GPU memory
  max_turns: 2
  learning_rate: 1.0e-4
  max_new_tokens: 256
  temperature: 0.7
  temperature_min: 0.7  # Min temp for rollout diversity
  temperature_max: 1.15  # Max temp for rollout diversity
  top_p: 0.95  # Base nucleus sampling threshold
  top_p_min: 0.85  # Min top_p for rollout diversity
  top_p_max: 0.99  # Max top_p for rollout diversity
  gamma: 0.99  # Discount factor for earlier turns (final turn gets full signal)
  rl_algo: "grpo"  # "reinforce" (fixed baseline) or "grpo" (group relative)
  max_grad_norm: 1.0  # Gradient clipping threshold
  shuffle: true  # Shuffle training samples each epoch
  async_pipeline: true  # Overlap judge with next question's rollouts (1-step delayed updates)
  debug_loss: false  # Log gradient accumulation diagnostics
  debug_judge: false  # Log full judge prompts and responses
  use_8bit_adam: false  # Disabled: causes KeyError 'state1' with LoRA (bitsandbytes bug)
  train_on_correct_only: false  # Train on all episodes (negative rewards for wrong/no-answer)

lora:
  enabled: true
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  gradient_checkpointing: true  # ~40% less VRAM, ~20% slower

checkpoint:
  save_every_n_steps: 5  # 0 = only final, N = every N steps
  save_new_checkpoint_every: 25  # Save checkpoint_{step} every N steps

judge:
  model: "models/Qwen3-8B-AWQ"
  base_url: "http://10.0.0.213:1234/v1"
  n_batch_judge: 1  # Max answers per judge call (1 = individual, >1 = batch similar answers)
  max_tokens: 1600  # Max output tokens for judge response (includes thinking)

reward:
  # Formula: sign from correctness, magnitude from approach score
  # - Correct: + approach/100 (good approach = high reward)
  # - Wrong:   - (1 - approach/100) (good approach = small penalty)
  use_approach_magnitude: true  # true = above formula, false = simple +1/-1
  skip_not_found_loss: true  # Skip loss for "Answer not found" responses (train turn 1 only)

environment:
  sandbox_host: "localhost"
  sandbox_port: 8080

logging:
  output_dir: "runs"
