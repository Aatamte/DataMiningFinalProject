# Full training run - production settings

model:
  name: "Qwen/Qwen3-4B-Instruct-2507"
  max_context: 3000  # Reduced from 3000 to save GPU memory
  use_tf32: true      # TF32 for Ampere GPUs (~10-15% faster)
  attention: "sdpa"   # "eager", "sdpa" (recommended), or "flash_attention_2"
  use_torch_compile: false  # torch.compile disabled (triton compatibility issues)

training:
  num_samples: 50
  num_epochs: 3
  num_rollouts: 6
  batch_splits: 1  # -1 = sequential, N = split rollouts into N batches for turn 1
  max_turns: 2
  learning_rate: 3.0e-6
  max_new_tokens: 256
  temperature: 0.7
  temperature_min: 0.7  # Min temp for rollout diversity
  temperature_max: 1.15  # Max temp for rollout diversity
  top_p: 0.95  # Base nucleus sampling threshold
  top_p_min: 0.85  # Min top_p for rollout diversity
  top_p_max: 0.99  # Max top_p for rollout diversity
  gamma: 0.99  # Discount factor for earlier turns (final turn gets full signal)
  correctness_weight: 0.75  # 75% correct answer, 25% approach quality
  lr_scheduler: "linear"  # "none" or "linear"
  lr_end: 3.0e-6  # Final LR for linear decay
  rl_algo: "grpo"  # "reinforce" (fixed baseline) or "grpo" (group relative)
  max_grad_norm: 1.0  # Gradient clipping threshold
  shuffle: true  # Shuffle training samples each epoch
  async_pipeline: true  # Overlap judge with next question's rollouts (1-step delayed updates)
  debug_loss: false  # Log gradient accumulation diagnostics
  debug_judge: false  # Log full judge prompts and responses
  use_8bit_adam: false  # Disabled: causes KeyError 'state1' with LoRA (bitsandbytes bug)

lora:
  enabled: true
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  gradient_checkpointing: true  # ~40% less VRAM, ~20% slower

checkpoint:
  save_every_n_steps: 5  # 0 = only final, N = every N steps

judge:
  model: "qwen3-8b"
  base_url: "http://localhost:1234/v1"

environment:
  sandbox_host: "localhost"
  sandbox_port: 8080

logging:
  output_dir: "runs"
