# Small training run - good for testing/validation

model:
  name: "Qwen/Qwen3-4B-Instruct-2507"
  max_context: 2048
  attention: "sdpa"   # "eager", "sdpa" (recommended), or "flash_attention_2"

training:
  num_samples: 10
  num_epochs: 2
  num_rollouts: 3
  gradient_micro_batches: 1  # 1 = disabled, N = split rollouts into N micro-batches for backward()
  max_turns: 3
  learning_rate: 1.0e-5
  max_new_tokens: 512
  temperature: 0.7
  temperature_min: 0.7
  temperature_max: 1.15
  gamma: 0.99  # Discount factor for earlier turns
  correctness_weight: 0.75  # 75% correct answer, 25% approach quality
  lr_scheduler: "linear"  # "none" or "linear"
  lr_end: 0.0  # Final LR for linear decay
  rl_algo: "grpo"  # "reinforce" (fixed baseline) or "grpo" (group relative)
  shuffle: true  # Shuffle training samples each epoch

lora:
  enabled: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

checkpoint:
  save_every_n_steps: 5  # 0 = only final, N = every N steps

judge:
  model: "deepseek/deepseek-r1-0528-qwen3-8b"
  base_url: "http://10.0.0.9:1234/v1"
  n_batch_judge: 1  # Max answers per judge call (1 = individual judging)

environment:
  sandbox_host: "localhost"
  sandbox_port: 8080

logging:
  output_dir: "runs"
